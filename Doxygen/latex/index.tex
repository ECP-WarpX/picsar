\section*{Particle-\/\+In-\/\+Cell Scalable Application Resource (P\+I\+C\+S\+AR) }

\subsection*{1. Overview }

The P\+I\+C\+S\+AR code is a \char`\"{}mini-\/app\char`\"{} standalone Particle-\/\+In-\/\+Cell (P\+IC) code that includes the key functionalities of the W\+A\+RP code main P\+IC loop. It is a compact {\bfseries self-\/contained proxy} that adequately portrays the computational loads and dataflow of the more complex W\+A\+RP code.

Since W\+A\+RP is a very large code written in a mix of F\+O\+R\+T\+R\+A\+N95, C and Python P\+I\+C\+S\+AR will be essential for studying multi-\/level parallelization on the next generation of exascale computers.

\paragraph*{A. Here are some of the specific algorithmic features of the P\+I\+C\+S\+AR code \+:}

The Maxwell solver uses arbitrary order finite-\/difference scheme (staggered/centered), The particle pusher uses the Boris algorithm, The field gathering routine is energy conserving, The current deposition and field gathering routines include high order particle shape factors.

\paragraph*{B. Here are some high performance features of the P\+I\+C\+S\+AR code \+:}

Particle tiling to help increase memory locality M\+PI parallelization for internode parallelism (blocking, non-\/blocking and Remote memory access M\+PI), Open\+MP parallelization for intranode parallelism, M\+P\+I-\/\+IO for fast parallel outputs. Vectorized subroutines (field gathering, classical current deposition)

\paragraph*{C. Python glue\+:}

We created a Forthon parser that read Fortran source files of P\+I\+C\+S\+AR and parse them to create a {\ttfamily picsar.\+v} file used by the Forthon compiler to generate a Python module for P\+I\+C\+S\+AR. The Forthon parser is available in the folder {\ttfamily utils}. Thanks to Forthon, we are able to access all high performance routines of P\+I\+C\+S\+AR from python. This allows us to use P\+I\+C\+S\+AR routines from W\+A\+RP and vice-\/versa.

\subsection*{2. Compiling }

\paragraph*{A. Python installation}

In order to install picsar in the form of a Python module, read detailed instructions in the file {\ttfamily I\+N\+S\+T\+A\+L\+L\+\_\+\+P\+Y\+T\+H\+O\+N.\+md}

\paragraph*{B. Fortran installation}

To build the code in full Fotran 90 read instructions from the file {\ttfamily I\+N\+S\+T\+A\+L\+L\+\_\+\+F\+O\+R\+T\+R\+A\+N.\+md}

\subsection*{3. Running simulations }

\paragraph*{A. Python mode}

An example of python script {\ttfamily test.\+py} is provided in {\ttfamily example\+\_\+scripts\+\_\+python}. To run this script in parallel, simply type \+: 
\begin{DoxyCode}
mpirun -np NMPI python test.py 
\end{DoxyCode}
 with N\+M\+PI the number of M\+PI processes.

\paragraph*{B. Fortran mode}

P\+I\+C\+S\+AR input parameters must be provided in an input file named {\ttfamily input\+\_\+file.\+pxr} in the folder where the code is ran. An example ({\ttfamily test.\+pxr}) of input file is provided in {\ttfamily example\+\_\+decks\+\_\+fortran/}. To run the executable on n M\+PI processes\+: 
\begin{DoxyCode}
mpirun -np n ./picsar
\end{DoxyCode}
 Notice that if {\ttfamily nprocx}, {\ttfamily nprocy} and {\ttfamily nprocz} are provided in the input file as part of the {\ttfamily cpusplit} section, then n must be equal to {\ttfamily nprocx x nprocy x nprocz} with {\ttfamily nprocx}, {\ttfamily nprocy}, {\ttfamily nprocz} the number of processors along x,y,z directions. Otherwise, if {\ttfamily nprocx}, {\ttfamily nprocy} and {\ttfamily nprocz} are not defined, the code performs automatic C\+PU split in each direction. User can specify some arguments in the command line. For the moments this feature supports only the number of tiles in each dimension and the init of particle distribution. Ex\+: {\ttfamily mpirun -\/np 1 ./picsar -\/ntilex ntx -\/ntiley nty -\/ntilez ntz -\/distr 1} with {\ttfamily ntx}, {\ttfamily nty} and {\ttfamily ntz} the number of tiles in each dimension (default is one) and distr the type of particle init (\char`\"{}1\char`\"{} for init on the x-\/axis of the grid and \char`\"{}2\char`\"{} for Random).

\paragraph*{C. Open\+MP}

If the code (Fortran or python version) was compiled with Open\+MP, you can set \char`\"{}x\char`\"{} Open\+MP threads per M\+PI task by defining {\ttfamily export O\+M\+P\+\_\+\+N\+U\+M\+\_\+\+T\+H\+R\+E\+A\+DS=x} before starting the simulation (default varies with the OS). Open\+MP scheduling for balancing loads between tiles can be adjusted at runtime by setting the environment variable {\ttfamily O\+M\+P\+\_\+\+S\+C\+H\+E\+D\+U\+LE} to either {\ttfamily static}, {\ttfamily guided} or {\ttfamily dynamic}. To ensure that threads have enough memory space on the stack, set {\ttfamily O\+M\+P\+\_\+\+S\+T\+A\+C\+K\+S\+I\+ZE} to high enough value. In practice, {\ttfamily export O\+M\+P\+\_\+\+S\+T\+A\+C\+K\+S\+I\+ZE=32M} should be sufficient for most of test cases.

\subsection*{4. Outputs }

\paragraph*{A. Field diagnostics}

For the moment, the code outputs binary matrix files with extensions \char`\"{}.\+pxr\char`\"{} that can be read using python scripts. Examples of such scripts are in the folder {\ttfamily postproc/}. In the Fortran version, the output frequency is controlled by setting the flag {\ttfamily output\+\_\+frequency} in the output section of the {\ttfamily input\+\_\+file.\+pixr}. Use {\ttfamily output\+\_\+frequency=-\/1} to disable outputs. The code places output files in a {\ttfamily R\+E\+S\+U\+L\+TS} directory where the code is ran. This directory has to be created before running the code in your submission script.

\paragraph*{B. Temporal diagnostics}

Temporal diagnosctics enable to outpout the time evolution of several physical quantities such as the species kinetic energies, the field energies and the L2 norm of divergence of the field minus the charge. Temporal diagnostics can be configurated using the section {\ttfamily temporal}. Output format can be controled using the keyword {\ttfamily format}. Temporal output files can be binary files ({\ttfamily format=1}) or ascii files ({\ttfamily format=1}). The output frequency can be controled via the keyword {\ttfamily frequency}. The different diagnostics can be activated with their flags\+:


\begin{DoxyItemize}
\item {\ttfamily kinE=1} for the kinetic energies
\item {\ttfamily exE=1}, {\ttfamily eyE=1}, {\ttfamily ezE=1}\+: electric field energies
\item {\ttfamily bxE=1}, {\ttfamily byE=1}, {\ttfamily bzE=1}\+: magnetic field energies
\item {\ttfamily div\+E-\/rho=1}\+: the L2 norm of divergence of the field minus the charge
\end{DoxyItemize}

\paragraph*{C. Time statistics}

Time statistics refer to the computation of the simulation times spend in each significant part of the code. A final time survey is automatically provided at the end of a simulation with the stand-\/alone code. The time statisctics function enables to outpout in files the time spend in the main subroutines for each iteration. It corresponds to the section named {\ttfamily timestat}. 