! ________________________________________________________________________________________
!
! *** Copyright Notice ***
!
! "Particle In Cell Scalable Application Resource (PICSAR) v2", Copyright (c)
! 2016, The Regents of the University of California, through Lawrence Berkeley
! National Laboratory (subject to receipt of any required approvals from the
! U.S. Dept. of Energy). All rights reserved.
!
! If you have questions about your rights to use or distribute this software, ! please contact Berkeley Lab's Innovation & Partnerships Office at IPO@lbl.gov.
!
! NOTICE.
! This Software was developed under funding from the U.S. Department of Energy
! and the U.S. Government consequently retains certain rights. As such, the U.S.
! Government has been granted for itself and others acting on its behalf a
! paid-up, nonexclusive, irrevocable, worldwide license in the Software to
! reproduce, distribute copies to the public, prepare derivative works, and
! perform publicly and display publicly, and to permit other to do so.
!
! SUBMAIN.F90
!
! This file is for the main PIC loop without initialization.
!
! Developers:
! Henri Vincenti
! Mathieu Lobet
!
! Date:
! Creation 2015
!
! Modifications:
! Mathieu Lobet - 2016 - Summary of the main parameters printed
!                        at the beginning of the simulation.
! Mathieu Lobet - 2016 - Creation of a partial 2D loop (does not include all 2d steps)
! ________________________________________________________________________________________

! ________________________________________________________________________________________
!> @brief
!> Subroutine that performs the time loop of the PIC algorithm.
!
!> @author
!> Henri Vincenti
!> Mathieu Lobet
!
!> @date
!> Creation 2015
!
!> @param[in] nst number of time steps
!
! ________________________________________________________________________________________
SUBROUTINE step(nst)

  USE constants
  USE fields
  USE particles
  USE params
  USE shared_data
  USE field_boundary
  USE particle_boundary
!  USE diagnostics
  USE simple_io
  USE sorting
  USE mpi_routines
#if (defined(VTUNE) && VTUNE>0)
  USE ITT_FORTRAN
#endif
#if (defined(SDE) && SDE>0)||(defined(DFP))
  USE SDE_FORTRAN
#endif

  IMPLICIT NONE
  INTEGER(idp) :: nst, i

  !!! --- This is the main PIC LOOP
  IF (rank .EQ. 0) THEN
    WRITE (0, *) "nsteps = ", nst
  END IF

  !!! --- Start Vtune/SDE analysis
#if VTUNE==1
  CALL start_vtune_collection()
#endif
#if SDE==1
  CALL start_sde_collection()
#endif
#if ALLINEA==1
  CALL ALLINEA_START_SAMPLING
#endif
  ! Intel Design Forward project
#if defined(DFP)
  CALL DFP_MAIN_START()
#endif

  ! ______________________________________________________________________________________
  !
  ! Main loop
  ! ______________________________________________________________________________________

  ! ___________________________________________
  ! Loop in 3D
  IF (c_dim.eq.3) THEN
    rhoold=0.0_num
    rho = 0.0_num
    DO i=1, nst
      IF (rank .EQ. 0) startit=MPI_WTIME()

      !!! --- Init iteration variables
      pushtime=0._num
      divE_computed = .False.

      IF (l_plasma) THEN
        !!! --- Field gather & particle push
        !IF (rank .EQ. 0) PRINT *, "#1"
        CALL field_gathering_plus_particle_pusher
        !!! --- Apply BC on particles
        CALL particle_bcs
        !IF (rank .EQ. 0) PRINT *, "#3"
        !!! --- Particle Sorting
        !write(0, *), 'Sorting'
        CALL pxr_particle_sorting
        !IF (rank .EQ. 0) PRINT *, "#4"
        !!! --- Deposit current of particle species on the grid
        !write(0, *), 'Depose currents'
        CALL pxrdepose_currents_on_grid_jxjyjz
        !IF (rank .EQ. 0) PRINT *, "#5"
        !!! --- Boundary conditions for currents
        !write(0, *), 'Current_bcs'
        CALL current_bcs
      ENDIF
        !IF (rank .EQ. 0) PRINT *, "#6"
        !!! --- Push B field half a time step
        !write(0, *), 'push_bfield'
        CALL push_bfield
        !IF (rank .EQ. 0) PRINT *, "#7"
        !!! --- Boundary conditions for B
        CALL bfield_bcs
        !IF (rank .EQ. 0) PRINT *, "#8"
        !!! --- Push E field  a full time step
        CALL push_efield
        !IF (rank .EQ. 0) PRINT *, "#9"
        !!! --- Boundary conditions for E
        CALL efield_bcs
        !IF (rank .EQ. 0) PRINT *, "#10"
        !!! --- push B field half a time step
        CALL push_bfield
        !IF (rank .EQ. 0) PRINT *, "#11"
        !!! --- Boundary conditions for B
        CALL bfield_bcs
      !IF (rank .EQ. 0) PRINT *, "#12"
      !!! --- Computes derived quantities
!      CALL calc_diags
      !IF (rank .EQ. 0) PRINT *, "#13"
      !!! --- Output simulation results
      CALL output_routines
      !IF (rank .EQ. 0) PRINT *, "#14"

      it = it+1
      timeit=MPI_WTIME()

      CALL time_statistics_per_iteration

      IF (rank .EQ. 0)  THEN
        WRITE(0, *) 'it = ', it, ' || time = ', it*dt, " || push/part (ns)= ",        &
        pushtime*1e9_num/ntot, " || tot/part (ns)= ", (timeit-startit)*1e9_num/ntot
      END IF
    END DO

    ! ___________________________________________
    ! Loop in 2D
  ELSE IF (c_dim.eq.2) THEN
     PRINT *, 'c_dim is 3, not 2'
     STOP
  ENDIF

  !!! --- Stop Vtune analysis
#if VTUNE==1
  CALL stop_vtune_collection()
#endif
#if SDE==1
  CALL stop_sde_collection()
#endif
#if ALLINEA==1
  CALL ALLINEA_STOP_SAMPLING
#endif
  ! Intel Design Forward project
#if defined(DFP)
  CALL DFP_MAIN_STOP
#endif

  ! Intel Design Forward project
#if defined(DFP)
  CALL DFP_FINAL_START
#endif

  !!! --- Output time statistics
!  CALL final_output_time_statistics

END SUBROUTINE step

! ________________________________________________________________________________________
!> @brief
!> Initialize the plasma and field arrays at it=0.
!
!> @author
!> Henri Vincenti
!> Mathieu Lobet
!
!> @date
!> Creation 2015
SUBROUTINE initall
  ! ________________________________________________________________________________________
  USE constants
  USE params
  USE fields
  USE particles
  USE shared_data
  USE tiling
  USE time_stat
  USE precomputed

  !use IFPORT ! uncomment if using the intel compiler (for rand)
  IMPLICIT NONE
  INTEGER(idp)                    :: ispecies, i
  REAL(num)                       :: tdeb
  TYPE(particle_species), POINTER :: curr
  TYPE(particle_dump), POINTER    :: dp

  ! Time statistics
  init_localtimes(:) = 0
  localtimes(:)=0

  ! Few calculations and updates
  nc    = nlab*g0! density (in the simulation frame)
  wlab  = echarge*sqrt(nlab/(emass*eps0))! plasma frequency (in the lab frame)
  lambdalab = 2*pi*clight/wlab
  w0_l  = echarge*sqrt(nc/(g0*emass*eps0))! "longitudinal" plasma frequency (in the lab frame)
  w0_t  = echarge*sqrt(nc/(g0**3*emass*eps0))! "transverse" plasma frequency (in the lab frame)
  w0    = w0_l

  !!! --- Set time step/ it
  IF (c_dim.eq.3) THEN
    IF (l_spectral) THEN
      dt=MIN(dx, dy, dz)/clight
    ELSE
      dt = dtcoef/(clight*sqrt(1.0_num/dx**2+1.0_num/dy**2+1.0_num/dz**2))
    ENDIF
  ENDIF
  it = 0

  !!! --- set number of time steps or total time
  if (nsteps .eq. 0) then
    nsteps = nint(tmax/(w0_l*dt))
  else
    tmax = nsteps*w0_l*dt
  endif

  !!! --- Sorting

  sorting_dx = sorting_dx*dx
  sorting_dy = sorting_dy*dy
  sorting_dz = sorting_dz*dz

  sorting_shiftx = sorting_shiftx*dx
  sorting_shifty = sorting_shifty*dy
  sorting_shiftz = sorting_shiftz*dz

  !!! --- Precomputed parameters
  dxi = 1.0_num/dx
  dyi = 1.0_num/dy
  dzi = 1.0_num/dz
  invvol = dxi*dyi*dzi
  dts2dx = 0.5_num*dt*dxi
  dts2dy = 0.5_num*dt*dyi
  dts2dz = 0.5_num*dt*dzi
  dtsdx0 = dt*dxi
  dtsdy0 = dt*dyi
  dtsdz0 = dt*dzi
  clightsq = 1.0_num/clight**2
  dxs2 = dx*0.5_num
  dys2 = dy*0.5_num
  dzs2 = dz*0.5_num

  ! Summary
  IF (rank .EQ. 0) THEN
    write(0, *) ''
    write(0, *) 'SIMULATION PARAMETERS:'
    write(0, *) 'Dimension:', c_dim
    write(0, *) 'dx, dy, dz:', dx, dy, dz
    write(0, *) 'dt:', dt, 's', dt*1e15, 'fs'
    write(0, '(" Coefficient on dt determined via the CFL (dtcoef): ", F12.5)')       &
    dtcoef
    write(0, *) 'Total time:', tmax, 'plasma periods:', tmax/w0_l, 's'
    write(0, *) 'Number of steps:', nsteps
    write(0, *) 'Tiles:', ntilex, ntiley, ntilez
    write(0, *) 'MPI com current:', mpicom_curr
    write(0, *) 'Field gathering method:', fieldgathe
    write(0, *) 'Field gathering plus particle pusher seperated:', fg_p_pp_separated
    write(0, *) 'Current/field gathering order:', nox, noy, noz
    write(0, '(" Particle communication: (partcom=", I1, ")")') partcom
    IF (particle_pusher.eq.1) THEN
      write(0, '(" Pusher: Jean-Luc Vay algorithm, particle_pusher=", I1)')           &
      particle_pusher
    ELSE
      write(0, '(" Pusher: Boris algorithm (particle_pusher=", I1, ")")')             &
      particle_pusher
    ENDIF
    write(0, *) 'MPI buffer size:', mpi_buf_size
    WRITE(0, *) ''
    write(0, *) 'PLASMA PROPERTIES:'
    write(0, *) 'Distribution:', pdistr
    write(0, *) 'Density in the lab frame:', nlab, 'm^-3'
    write(0, *) 'Density in the simulation frame:', nc, 'm^-3'
    write(0, *) 'Cold plasma frequency in the lab frame:', wlab, 's^-1'
    write(0, *) 'cold plasma wavelength:', lambdalab, 'm', lambdalab*1e6, 'um'
    write(0, *) ''

    write(0, '(" MPI domain decomposition")')
    write(0, *) 'Topology:', topology
    write(0, '(" Local number of cells:", I5, X, I5, X, I5)') nx, ny, nz
    write(0, '(" Local number of grid point:", I5, X, I5, X, I5)') nx_grid, ny_grid,  &
    nz_grid
    write(0, '(" Guard cells:", I5, X, I5, X, I5)') nxguards, nyguards, nzguards
    write(0, *) ''
    write(0, '(" FFTW - parameters ")')
    ! Sorting
    IF (sorting_activated.gt.0) THEN
      write(0, *) 'Particle sorting activated'
      write(0, *) 'dx:', sorting_dx
      write(0, *) 'dy:', sorting_dy
      write(0, *) 'dz:', sorting_dz
      write(0, *) 'shiftx:', sorting_shiftx
      write(0, *) 'shifty:', sorting_shifty
      write(0, *) 'shiftz:', sorting_shiftz
      write(0, *) ''
    ELSE
      write(0, *) 'Particle sorting non-activated'
      write(0, *) ''
    ENDIF

    ! Species properties
    write(0, *)  'Number of species:', nspecies
    DO ispecies=1, nspecies
      curr => species_parray(ispecies)
      write(0, *) trim(adjustl(curr%name))
      write(0, *) 'Charge:', curr%charge
      write(0, *) 'Drift velocity:', curr%vdrift_x, curr%vdrift_y, curr%vdrift_z
      write(0, *) 'Sorting period:', curr%sorting_period
      write(0, *) 'Sorting start:', curr%sorting_start
      write(0, *) ''
    end do

    ! Diags
    IF (timestat_activated.gt.0) THEN
      write(0, *) 'Output of time statistics activated'
      write(0, *) 'Computation of the time statistics starts at', timestat_itstart
      write(0, *) 'Buffer size:', nbuffertimestat
    ELSE
      write(0, *) 'Output of time statistics non-activated'
      write(0, '(X, "Computation of the time statistics starts at iteration:", I5)')  &
      timestat_itstart
    ENDIF
    write(0, *)

    ! Particle Dump
    IF (npdumps.gt.0) THEN
      DO i = 1, npdumps
        dp => particle_dumps(i)
        WRITE(0, '(" Dump number: ", I2)') i
        WRITE(0, '(" species name: ", A10)') species_parray(dp%ispecies)%name
        WRITE(0, *)
      ENDDO
    ELSE
      WRITE(0, '(" No particle dump (", I2, ")")') npdumps
      WRITE(0, *)
    ENDIF

  end if

  ! ------ INIT PARTICLE DISTRIBUTIONS

  tdeb=MPI_WTIME()

  !!! --- Set tile split for particles
  CALL set_tile_split

  IF (rank .EQ. 0) write(0, *) "Set tile split: done"

  ! - Allocate particle arrays for each tile of each species
  CALL init_tile_arrays

  IF (rank .EQ. 0) write(0, *) "Initialization of the tile arrays: done"

  ! - Load particle distribution on each tile
  CALL load_particles

  IF (rank .EQ. 0) write(0, *) "Creation of the particles: done"

  init_localtimes(1) = MPI_WTIME() - tdeb

  ! - Estimate tile size
  CALL estimate_total_memory_consumption

  ! ----- INIT FIELD ARRAYS
  !!! --- Initialize field/currents arrays
  ! - Init grid arrays
  ex=0.0_num;ey=0.0_num;ez=0.0_num
  bx=0.0_num;by=0.0_num;bz=0.0_num
  jx=0.0_num;jy=0.0_num;jz=0.0_num
END SUBROUTINE initall

! ________________________________________________________________________________________
!> @brief
!> Subroutine that computes total memory consumption (particle/grid tile structures and 
!> regular grid arrays used in the Maxwell solver)
!
!> @author
!> Henri Vincenti
!
!> @date
!> Creation 2018
! ________________________________________________________________________________________
SUBROUTINE estimate_total_memory_consumption
  USE tiling
  USE mpi_routines
  USE mem_status
  IMPLICIT NONE 
  REAL(num) :: total_memory=0._num, avg_per_mpi=0._num
  ! - Get local/global memory occupied by tile arrays (grid and particles)
  CALL get_local_tile_mem()
  CALL get_global_tile_mem()

  ! - Get local/global memory occupied by grid arrays (Maxwell solver)
  CALL get_local_grid_mem()
  CALL get_global_grid_mem()

  ! - Output results on standard output 
  IF (rank .EQ. 0) THEN 
    total_memory=global_grid_mem+global_grid_tiles_mem+global_part_tiles_mem
    avg_per_mpi=total_memory/nproc
    WRITE(0, *) 'Total memory (GB) for grid arrays ', global_grid_mem/1e9
    WRITE(0, *) 'Total memory (GB) for grid tile arrays ', global_grid_tiles_mem/1e9
    WRITE(0, *) 'Total memory (GB) for particle tile arrays ', global_part_tiles_mem/1e9
    WRITE(0, *) 'Total memory (GB)', total_memory/1e9
    WRITE(0, *) 'Avg memory (GB) /MPI process ', avg_per_mpi/1e9
  ENDIF 
END SUBROUTINE estimate_total_memory_consumption

